{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13edd81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import GPT2Config, GPT2Tokenizer, BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from utils.InductiveAttentionModels import GPT2InductiveAttentionHeadModel\n",
    "from utils.SequenceCrossEntropyLoss import SequenceCrossEntropyLoss\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import tqdm\n",
    "import os\n",
    "import string\n",
    "\n",
    "# from nltk.translate import bleu_score\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d5b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert_model_recall = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "bert_model_rerank = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2InductiveAttentionHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# REC_TOKEN = \"R\"\n",
    "# REC_END_TOKEN = \"E\"\n",
    "REC_TOKEN = \"[REC]\"\n",
    "REC_END_TOKEN = \"[REC_END]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "PLACEHOLDER_TOKEN = \"[MOVIE_ID]\"\n",
    "gpt_tokenizer.add_tokens([REC_TOKEN, REC_END_TOKEN, SEP_TOKEN, PLACEHOLDER_TOKEN])\n",
    "gpt2_model.resize_token_embeddings(len(gpt_tokenizer)) \n",
    "original_token_emb_size = gpt2_model.get_input_embeddings().weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea32971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieRecDataset(Dataset):\n",
    "    def __init__(self, data, bert_tok, gpt2_tok):\n",
    "        self.data = data\n",
    "        self.bert_tok = bert_tok\n",
    "        self.gpt2_tok = gpt2_tok\n",
    "        self.turn_ending = torch.tensor([[628, 198]]) # end of turn, '\\n\\n\\n'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        dialogue = self.data[index]\n",
    "        \n",
    "        dialogue_tokens = []\n",
    "        \n",
    "        for utterance, gt_ind in dialogue:\n",
    "            utt_tokens = self.gpt2_tok(utterance, return_tensors=\"pt\")['input_ids']\n",
    "            dialogue_tokens.append( ( torch.cat( (utt_tokens, self.turn_ending), dim=1), gt_ind) )\n",
    "            \n",
    "        role_ids = None\n",
    "        previous_role_ids = None\n",
    "        if role_ids == None:\n",
    "            role_ids = [ 0 if item[0] == 'B' else 1 for item, _ in dialogue]\n",
    "            previous_role_ids = role_ids\n",
    "        else:\n",
    "            role_ids = [ 0 if item[0] == 'B' else 1 for item, _ in dialogue]\n",
    "            if not np.array_equal(role_ids, previous_role_ids):\n",
    "                raise Exception(\"Role ids dont match between languages\")\n",
    "            previous_role_ids = role_ids\n",
    "        \n",
    "        return role_ids, dialogue_tokens\n",
    "    \n",
    "    def collate(self, unpacked_data):\n",
    "        return unpacked_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2372a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/local-scratch1/data/by2299/redial_full_train_placeholder\"\n",
    "test_path = \"/local-scratch1/data/by2299/redial_full_test_placeholder\"\n",
    "items_db_path = \"/local-scratch1/data/by2299/redial_full_movie_db_placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24e3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieRecDataset(torch.load(train_path), bert_tokenizer, gpt_tokenizer)\n",
    "test_dataset = MovieRecDataset(torch.load(test_path), bert_tokenizer, gpt_tokenizer)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, shuffle=False, batch_size=1, collate_fn=train_dataset.collate)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=1, collate_fn=test_dataset.collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ab9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_db = torch.load(items_db_path)\n",
    "\n",
    "def sample_ids_from_db(item_db,\n",
    "                       gt_id, # ground truth id\n",
    "                       num_samples, # num samples to return\n",
    "                       include_gt # if we want gt_id to be included\n",
    "                      ):\n",
    "    ids_2_sample_from = list(item_db.keys())\n",
    "    ids_2_sample_from.remove(gt_id)\n",
    "    if include_gt:\n",
    "        results = random.sample(ids_2_sample_from, num_samples-1)\n",
    "        results.append(gt_id)\n",
    "    else:\n",
    "        results = random.sample(ids_2_sample_from, num_samples)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57343dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalCRSModel(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 language_model, # backbone of Pretrained LM such as GPT2\n",
    "                 encoder, # backbone of item encoder such as bert\n",
    "                 recall_encoder,\n",
    "                 lm_tokenizer, # language model tokenizer\n",
    "                 encoder_tokenizer, # item encoder tokenizer\n",
    "                 device, # Cuda device\n",
    "                 items_db, # {id:info}, information of all items to be recommended\n",
    "                 annoy_base_recall=None, # annoy index base of encoded recall embeddings of items\n",
    "                 annoy_base_rerank=None, # annoy index base of encoded rerank embeddings of items, for validation and inference only\n",
    "                 recall_item_dim=768, # dimension of each item to be stored in annoy base\n",
    "                 lm_trim_offset=100, # offset to trim language model wte inputs length = (1024-lm_trim_offset)\n",
    "                 rec_token_str=\"[REC]\", # special token indicating recommendation and used for recall\n",
    "                 rec_end_token_str=\"[REC_END]\", # special token indicating recommendation ended, conditional generation starts\n",
    "                 sep_token_str=\"[SEP]\",\n",
    "                 placeholder_token_str=\"[MOVIE_ID]\"\n",
    "                ):\n",
    "        super(UniversalCRSModel, self).__init__()\n",
    "        \n",
    "        #models and tokenizers\n",
    "        self.language_model = language_model\n",
    "        self.item_encoder = encoder\n",
    "        self.recall_encoder = recall_encoder\n",
    "        self.lm_tokenizer = lm_tokenizer\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        # item db and annoy index base\n",
    "        self.items_db = items_db\n",
    "        self.annoy_base_recall = annoy_base_recall\n",
    "        self.annoy_base_rerank = annoy_base_rerank\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.recall_item_dim = recall_item_dim\n",
    "        self.lm_trim_offset = lm_trim_offset\n",
    "        \n",
    "        #constants\n",
    "        self.REC_TOKEN_STR = rec_token_str\n",
    "        self.REC_END_TOKEN_STR = rec_end_token_str\n",
    "        self.SEP_TOKEN_STR = sep_token_str\n",
    "        self.PLACEHOLDER_TOKEN_STR = placeholder_token_str\n",
    "        \n",
    "        # map language model hidden states to a vector to query annoy-item-base for recall\n",
    "        self.recall_lm_query_mapper = torch.nn.Linear(self.language_model.config.n_embd, self.recall_item_dim) # default [768,768]\n",
    "        # map output of self.item_encoder to vectors to be stored in annoy-item-base \n",
    "        self.recall_item_wte_mapper = torch.nn.Linear(self.recall_encoder.config.hidden_size, self.recall_item_dim) # default [768,768]\n",
    "        # map output of self.item_encoder to a wte of self.language_model\n",
    "        self.rerank_item_wte_mapper = torch.nn.Linear(self.item_encoder.config.hidden_size, self.language_model.config.n_embd) # default [768,768]\n",
    "        # map language model hidden states of item wte to a one digit logit for softmax computation\n",
    "        self.rerank_logits_mapper = torch.nn.Linear(self.language_model.config.n_embd, 1) # default [768,1]\n",
    "    \n",
    "    def get_sep_token_wtes(self):\n",
    "        sep_token_input_ids = self.lm_tokenizer(self.SEP_TOKEN_STR, return_tensors=\"pt\")[\"input_ids\"].to(self.device)\n",
    "        return self.language_model.transformer.wte(sep_token_input_ids) # [1, 1, self.language_model.config.n_embd]\n",
    "\n",
    "    def get_rec_token_wtes(self):\n",
    "        rec_token_input_ids = self.lm_tokenizer(self.REC_TOKEN_STR, return_tensors=\"pt\")[\"input_ids\"].to(self.device)\n",
    "        return self.language_model.transformer.wte(rec_token_input_ids) # [1, 1, self.language_model.config.n_embd]\n",
    "    \n",
    "    def get_rec_end_token_wtes(self):\n",
    "        rec_end_token_input_ids = self.lm_tokenizer(self.REC_END_TOKEN_STR, return_tensors=\"pt\")[\"input_ids\"].to(self.device)\n",
    "        return self.language_model.transformer.wte(rec_end_token_input_ids) # [1, 1, self.language_model.config.n_embd]\n",
    "    \n",
    "    def get_movie_title(self, m_id):\n",
    "        title = self.items_db[m_id]\n",
    "        title = title.split('[SEP]')[0].strip()\n",
    "        return title\n",
    "    \n",
    "    # compute BERT encoded item hidden representation\n",
    "    # output can be passed to self.recall_item_wte_mapper or self.rerank_item_wte_mapper\n",
    "    def compute_encoded_embeddings_for_items(self, \n",
    "                                             encoder_to_use,\n",
    "                                             item_ids, # an array of ids, single id should be passed as [id]\n",
    "                                             items_db_to_use # item databse to use\n",
    "                                            ):\n",
    "        chunk_ids = item_ids\n",
    "        chunk_infos = [items_db_to_use[key] for key in chunk_ids ]\n",
    "        chunk_tokens = self.encoder_tokenizer(chunk_infos, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        chunk_input_ids = chunk_tokens['input_ids'].to(self.device)\n",
    "        chunk_attention_mask = chunk_tokens['attention_mask'].to(self.device)\n",
    "        chunk_hiddens = encoder_to_use(input_ids=chunk_input_ids, attention_mask=chunk_attention_mask).last_hidden_state\n",
    "\n",
    "        # average of non-padding tokens\n",
    "        expanded_mask_size = list(chunk_attention_mask.size())\n",
    "        expanded_mask_size.append(encoder_to_use.config.hidden_size)\n",
    "        expanded_mask = chunk_attention_mask.unsqueeze(-1).expand(expanded_mask_size)\n",
    "        chunk_masked = torch.mul(chunk_hiddens, expanded_mask) # [num_example, len, 768]\n",
    "        chunk_pooled = torch.sum(chunk_masked, dim=1) / torch.sum(chunk_attention_mask, dim=1).unsqueeze(-1)\n",
    "        \n",
    "        # [len(item_ids), encoder_to_use.config.hidden_size], del chunk_hiddens to free up GPU memory\n",
    "        return chunk_pooled, chunk_hiddens\n",
    "    \n",
    "    # annoy_base_constructor, constructs\n",
    "    def annoy_base_constructor(self, items_db=None, distance_type='angular', chunk_size=50, n_trees=10):\n",
    "        items_db_to_use = self.items_db if items_db == None else items_db\n",
    "        all_item_ids = list(items_db_to_use.keys())\n",
    "        \n",
    "        total_pooled = []\n",
    "        # break into chunks/batches for model concurrency\n",
    "        num_chunks = math.ceil(len(all_item_ids) / chunk_size)\n",
    "        for i in range(num_chunks):\n",
    "            chunk_ids = all_item_ids[i*chunk_size: (i+1)*chunk_size]\n",
    "            chunk_pooled, chunk_hiddens = self.compute_encoded_embeddings_for_items(self.recall_encoder, chunk_ids, items_db_to_use)\n",
    "            chunk_pooled = chunk_pooled.cpu().detach().numpy()\n",
    "            del chunk_hiddens\n",
    "            total_pooled.append(chunk_pooled)\n",
    "        total_pooled = np.concatenate(total_pooled, axis=0)\n",
    "        \n",
    "        pooled_tensor = torch.tensor(total_pooled).to(self.device)\n",
    "        \n",
    "        #build recall annoy index\n",
    "        annoy_base_recall = AnnoyIndex(self.recall_item_wte_mapper.out_features, distance_type)\n",
    "        pooled_recall = self.recall_item_wte_mapper(pooled_tensor) # [len(items_db_to_use), self.recall_item_wte_mapper.out_features]\n",
    "        pooled_recall = pooled_recall.cpu().detach().numpy()\n",
    "        for i, vector in zip(all_item_ids, pooled_recall):\n",
    "            annoy_base_recall.add_item(i, vector)\n",
    "        annoy_base_recall.build(n_trees)\n",
    "        \n",
    "        total_pooled = []\n",
    "        # break into chunks/batches for model concurrency\n",
    "        num_chunks = math.ceil(len(all_item_ids) / chunk_size)\n",
    "        for i in range(num_chunks):\n",
    "            chunk_ids = all_item_ids[i*chunk_size: (i+1)*chunk_size]\n",
    "            chunk_pooled, chunk_hiddens = self.compute_encoded_embeddings_for_items(self.item_encoder, chunk_ids, items_db_to_use)\n",
    "            chunk_pooled = chunk_pooled.cpu().detach().numpy()\n",
    "            del chunk_hiddens\n",
    "            total_pooled.append(chunk_pooled)\n",
    "        total_pooled = np.concatenate(total_pooled, axis=0)\n",
    "        \n",
    "        pooled_tensor = torch.tensor(total_pooled).to(self.device)\n",
    "        \n",
    "        #build rerank annoy index, for validation and inference only\n",
    "        annoy_base_rerank = AnnoyIndex(self.rerank_item_wte_mapper.out_features, distance_type)\n",
    "        pooled_rerank = self.rerank_item_wte_mapper(pooled_tensor) # [len(items_db_to_use), self.recall_item_wte_mapper.out_features]\n",
    "        pooled_rerank = pooled_rerank.cpu().detach().numpy()\n",
    "        for i, vector in zip(all_item_ids, pooled_rerank):\n",
    "            annoy_base_rerank.add_item(i, vector)\n",
    "        annoy_base_rerank.build(n_trees)\n",
    "        \n",
    "        del pooled_tensor\n",
    "        \n",
    "        self.annoy_base_recall = annoy_base_recall\n",
    "        self.annoy_base_rerank = annoy_base_rerank\n",
    "    \n",
    "    def annoy_loader(self, path, annoy_type, distance_type=\"angular\"):\n",
    "        if annoy_type == \"recall\":\n",
    "            annoy_base = AnnoyIndex(self.recall_item_wte_mapper.out_features, distance_type)\n",
    "            annoy_base.load(path)\n",
    "            return annoy_base\n",
    "        elif annoy_type == \"rerank\":\n",
    "            annoy_base = AnnoyIndex(self.rerank_item_wte_mapper.out_features, distance_type)\n",
    "            annoy_base.load(path)\n",
    "            return annoy_base\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def lm_expand_wtes_with_items_annoy_base(self):\n",
    "        all_item_ids = list(self.items_db.keys())\n",
    "        total_pooled = []\n",
    "        for i in all_item_ids:\n",
    "            total_pooled.append(self.annoy_base_rerank.get_item_vector(i))\n",
    "        total_pooled = np.asarray(total_pooled) # [len(all_item_ids), 768]\n",
    "        pooled_tensor = torch.tensor(total_pooled, dtype=torch.float).to(self.device)\n",
    "        \n",
    "        old_embeddings = self.language_model.get_input_embeddings()\n",
    "        item_id_2_lm_token_id = {}\n",
    "        for k in all_item_ids:\n",
    "            item_id_2_lm_token_id[k] = len(item_id_2_lm_token_id) + old_embeddings.weight.shape[0]\n",
    "        new_embeddings = torch.cat([old_embeddings.weight, pooled_tensor], 0)\n",
    "        new_embeddings = torch.nn.Embedding.from_pretrained(new_embeddings)\n",
    "        self.language_model.set_input_embeddings(new_embeddings)\n",
    "        self.language_model.to(device)\n",
    "        return item_id_2_lm_token_id\n",
    "    \n",
    "    def lm_restore_wtes(self, original_token_emb_size):\n",
    "        old_embeddings = self.language_model.get_input_embeddings()\n",
    "        new_embeddings = torch.nn.Embedding(original_token_emb_size, old_embeddings.weight.size()[1])\n",
    "        new_embeddings.to(self.device, dtype=old_embeddings.weight.dtype)\n",
    "        new_embeddings.weight.data[:original_token_emb_size, :] = old_embeddings.weight.data[:original_token_emb_size, :]\n",
    "        self.language_model.set_input_embeddings(new_embeddings)\n",
    "        self.language_model.to(self.device)\n",
    "        assert(self.language_model.get_input_embeddings().weight.shape[0] == original_token_emb_size)\n",
    "    \n",
    "    def trim_lm_wtes(self, wtes):\n",
    "        trimmed_wtes = wtes\n",
    "        if trimmed_wtes.shape[1] > self.language_model.config.n_positions:\n",
    "            trimmed_wtes = trimmed_wtes[:,-self.language_model.config.n_positions + self.lm_trim_offset:,:]\n",
    "        return trimmed_wtes # [batch, self.language_model.config.n_positions - self.lm_trim_offset, self.language_model.config.n_embd]\n",
    "    \n",
    "    def trim_positional_ids(self, p_ids, num_items_wtes):\n",
    "        trimmed_ids = p_ids\n",
    "        if trimmed_ids.shape[1] > self.language_model.config.n_positions:\n",
    "            past_ids = trimmed_ids[:,:self.language_model.config.n_positions - self.lm_trim_offset - num_items_wtes]\n",
    "#             past_ids = trimmed_ids[:, self.lm_trim_offset + num_items_wtes:self.language_model.config.n_positions]\n",
    "            item_ids = trimmed_ids[:,-num_items_wtes:]\n",
    "            trimmed_ids = torch.cat((past_ids, item_ids), dim=1)\n",
    "        return trimmed_ids # [batch, self.language_model.config.n_positions - self.lm_trim_offset]\n",
    "    \n",
    "    def compute_inductive_attention_mask(self, length_language, length_rerank_items_wtes):\n",
    "        total_length = length_language + length_rerank_items_wtes\n",
    "        language_mask_to_add = torch.zeros((length_language, total_length), dtype=torch.float, device=self.device)\n",
    "        items_mask_to_add = torch.ones((length_rerank_items_wtes, total_length), dtype=torch.float, device=self.device)\n",
    "        combined_mask_to_add = torch.cat((language_mask_to_add, items_mask_to_add), dim=0)\n",
    "        return combined_mask_to_add #[total_length, total_length]\n",
    "    \n",
    "    def forward_pure_language_turn(self, \n",
    "                                   past_wtes, # past word token embeddings, [1, len, 768]\n",
    "                                   current_tokens # tokens of current turn conversation, [1, len]\n",
    "                                  ):\n",
    "        train_logits, train_targets = None, None\n",
    "        current_wtes = self.language_model.transformer.wte(current_tokens)\n",
    "        \n",
    "        if past_wtes == None:\n",
    "            lm_outputs = self.language_model(inputs_embeds=current_wtes)\n",
    "            train_logits = lm_outputs.logits[:, :-1, :]\n",
    "            train_targets = current_tokens[:,1:]\n",
    "        else:\n",
    "            all_wtes = torch.cat((past_wtes, current_wtes), dim=1)\n",
    "            all_wtes = self.trim_lm_wtes(all_wtes)\n",
    "            lm_outputs = self.language_model(inputs_embeds=all_wtes)\n",
    "            train_logits = lm_outputs.logits[:, -current_wtes.shape[1]:-1, :] # skip the last one\n",
    "            train_targets = current_tokens[:,1:]\n",
    "        \n",
    "        # torch.Size([batch, len_cur, lm_vocab]), torch.Size([batch, len_cur]), torch.Size([batch, len_past+len_cur, lm_emb(768)])\n",
    "        return train_logits, train_targets\n",
    "        \n",
    "    def forward_recall(self, \n",
    "                       past_wtes, # past word token embeddings, [1, len, 768]\n",
    "                       current_tokens, # tokens of current turn conversation, [1, len]\n",
    "                       gt_item_id, # id, ex. 0\n",
    "                       num_samples # num examples to sample for training, including groud truth id\n",
    "                      ):\n",
    "        # recall step 1. construct LM sequence output\n",
    "        # LM input composition: [past_wtes, REC_wtes, gt_item_wte, (gt_item_info_wtes), REC_END_wtes, current_wtes ]\n",
    "        \n",
    "        REC_wtes = self.get_rec_token_wtes() # [1, 1, self.language_model.config.n_embd]\n",
    "        gt_item_wte, _ = self.compute_encoded_embeddings_for_items(self.recall_encoder, [gt_item_id], self.items_db)\n",
    "        gt_item_wte = self.rerank_item_wte_mapper(gt_item_wte) # [1, self.rerank_item_wte_mapper.out_features]\n",
    "        \n",
    "        REC_END_wtes = self.get_rec_end_token_wtes() # [1, 1, self.language_model.config.n_embd]\n",
    "        current_wtes = self.language_model.transformer.wte(current_tokens) #[1, current_tokens.shape[1], self.language_model.config.n_embd]\n",
    "        \n",
    "        REC_wtes_len = REC_wtes.shape[1] # 1 by default\n",
    "        gt_item_wte_len = gt_item_wte.shape[0] # 1 by default\n",
    "        REC_END_wtes_len = REC_END_wtes.shape[1] # 1 by default\n",
    "        current_wtes_len = current_wtes.shape[1]\n",
    "        \n",
    "        lm_wte_inputs = torch.cat(\n",
    "            (past_wtes, # [batch (1), len, self.language_model.config.n_embd]\n",
    "             REC_wtes,\n",
    "             gt_item_wte.unsqueeze(0), # reshape to [1,1,self.rerank_item_wte_mapper.out_features]\n",
    "             REC_END_wtes,\n",
    "             current_wtes # [batch (1), len, self.language_model.config.n_embd]\n",
    "            ),\n",
    "            dim=1\n",
    "        )\n",
    "        lm_wte_inputs = self.trim_lm_wtes(lm_wte_inputs) # trim for len > self.language_model.config.n_positions\n",
    "        \n",
    "        # recall step 2. get gpt output logits and hidden states\n",
    "        lm_outputs = self.language_model(inputs_embeds=lm_wte_inputs, output_hidden_states=True)\n",
    "        \n",
    "        # recall step 3. pull logits (recall, rec_token and language logits of current turn) and compute\n",
    "        \n",
    "        # recall logit(s)\n",
    "        rec_token_start_index = -current_wtes_len-REC_END_wtes_len-gt_item_wte_len-REC_wtes_len\n",
    "        rec_token_end_index = -current_wtes_len-REC_END_wtes_len-gt_item_wte_len\n",
    "        # [batch (1), REC_wtes_len, self.language_model.config.n_embd]\n",
    "        rec_token_hidden = lm_outputs.hidden_states[-1][:, rec_token_start_index:rec_token_end_index, :]\n",
    "        # [batch (1), self.recall_lm_query_mapper.out_features]\n",
    "        rec_query_vector = self.recall_lm_query_mapper(rec_token_hidden).squeeze(1)\n",
    "        \n",
    "        # sample num_samples item ids to train recall with \"recommendation as classification\"\n",
    "        sampled_item_ids = sample_ids_from_db(self.items_db, gt_item_id, num_samples, include_gt=True)\n",
    "        gt_item_id_index = sampled_item_ids.index(gt_item_id)\n",
    "        \n",
    "        # [num_samples, self.item_encoder.config.hidden_size]\n",
    "        encoded_items_embeddings, _ = self.compute_encoded_embeddings_for_items(self.recall_encoder, sampled_item_ids, self.items_db)\n",
    "        # to compute dot product with rec_query_vector\n",
    "        items_key_vectors = self.recall_item_wte_mapper(encoded_items_embeddings) # [num_samples, self.recall_item_wte_mapper.out_features]\n",
    "        expanded_rec_query_vector = rec_query_vector.expand(items_key_vectors.shape[0], rec_query_vector.shape[1]) # [num_samples, self.recall_item_wte_mapper.out_features]\n",
    "        recall_logits = torch.sum(expanded_rec_query_vector * items_key_vectors, dim=1) # torch.size([num_samples])\n",
    "        \n",
    "        # REC_TOKEN prediction and future sentence prediction\n",
    "        # hidden rep of the token that's right before REC_TOKEN\n",
    "        token_before_REC_logits = lm_outputs.logits[:, rec_token_start_index-1:rec_token_end_index-1, :]\n",
    "        REC_targets = self.lm_tokenizer(self.REC_TOKEN_STR, return_tensors=\"pt\")['input_ids'].to(self.device) # [1, 1]\n",
    "        \n",
    "        #language logits and targets\n",
    "        current_language_logits = lm_outputs.logits[:, -current_wtes_len:-1, :]\n",
    "        current_language_targets = current_tokens[:,1:]\n",
    "        \n",
    "        # REC token and language, their logits and targets\n",
    "        # [batch, current_wtes_len+REC_wtes_len, lm_vocab]\n",
    "        all_wte_logits = torch.cat((token_before_REC_logits, current_language_logits), dim=1)\n",
    "        # [current_wtes_len+REC_wtes_len, lm_vocab]\n",
    "        all_wte_targets = torch.cat((REC_targets, current_language_targets), dim=1)\n",
    "        \n",
    "        # torch.size([num_samples]), id, [batch, current_wtes_len+REC_wtes_len, lm_vocab], [current_wtes_len+REC_wtes_len, lm_vocab]\n",
    "        return recall_logits, gt_item_id_index, all_wte_logits, all_wte_targets\n",
    "        \n",
    "    def forward_rerank(self,\n",
    "                       past_wtes, # past word token embeddings, [1, len, 768]\n",
    "                       gt_item_id, # tokens of current turn conversation, [1, len]\n",
    "                       num_samples, # num examples to sample for training, including groud truth id\n",
    "                       rerank_items_chunk_size=10, # batch size for encoder GPU computation\n",
    "                      ):\n",
    "        # REC wte\n",
    "        REC_wtes = self.get_rec_token_wtes() # [batch (1), 1, self.language_model.config.n_embd]\n",
    "        \n",
    "        #  items wtes to compute rerank loss\n",
    "        # sample rerank examples\n",
    "        sampled_item_ids = sample_ids_from_db(self.items_db, gt_item_id, num_samples, include_gt=True)\n",
    "        gt_item_id_index = sampled_item_ids.index(gt_item_id)\n",
    "        # compute item wtes by batch\n",
    "        num_chunks = math.ceil(len(sampled_item_ids) / rerank_items_chunk_size)\n",
    "        total_wtes = []\n",
    "        for i in range(num_chunks):\n",
    "            chunk_ids = sampled_item_ids[i*rerank_items_chunk_size: (i+1)*rerank_items_chunk_size]\n",
    "            chunk_pooled, _ = self.compute_encoded_embeddings_for_items(self.item_encoder, chunk_ids, self.items_db) # [rerank_items_chunk_size, self.item_encoder.config.hidden_size]\n",
    "            chunk_wtes = self.rerank_item_wte_mapper(chunk_pooled)\n",
    "            total_wtes.append(chunk_wtes)\n",
    "        total_wtes = torch.cat(total_wtes, dim=0) # [num_samples, self.language_model.config.n_embd]\n",
    "        \n",
    "        past_wtes_len = past_wtes.shape[1]\n",
    "        REC_wtes_len = REC_wtes.shape[1] # 1 by default\n",
    "        total_wtes_len = total_wtes.shape[0]\n",
    "        \n",
    "        # compute positional ids, all rerank item wte should have the same positional encoding id 0\n",
    "        position_ids = torch.arange(0, past_wtes_len + REC_wtes_len, dtype=torch.long, device=self.device)\n",
    "        items_position_ids = torch.zeros(total_wtes.shape[0], dtype=torch.long, device=device)\n",
    "#         items_position_ids = torch.tensor([1023] * total_wtes.shape[0], dtype=torch.long, device=device)\n",
    "        combined_position_ids = torch.cat((position_ids, items_position_ids), dim=0)\n",
    "        combined_position_ids = combined_position_ids.unsqueeze(0) # [1, past_wtes_len+REC_wtes_len+total_wtes_len]\n",
    "        \n",
    "        # compute concatenated lm wtes\n",
    "        lm_wte_inputs = torch.cat(\n",
    "            (past_wtes, # [batch (1), len, self.language_model.config.n_embd]\n",
    "             REC_wtes, # [batch (1), 1, self.language_model.config.n_embd]\n",
    "             total_wtes.unsqueeze(0), # [1, num_samples, self.language_model.config.n_embd]\n",
    "            ),\n",
    "            dim=1\n",
    "        ) # [1, past_len + REC_wtes_len + num_samples, self.language_model.config.n_embd]\n",
    "\n",
    "        # trim sequence to smaller length (len < self.language_model.config.n_positions-self.lm_trim_offset)\n",
    "        combined_position_ids_trimmed = self.trim_positional_ids(combined_position_ids, total_wtes_len) # [1, len]\n",
    "        lm_wte_inputs_trimmed = self.trim_lm_wtes(lm_wte_inputs) # [1, len, self.language_model.config.n_embd]\n",
    "        assert(combined_position_ids.shape[1] == lm_wte_inputs.shape[1])\n",
    "        \n",
    "        # compute inductive attention mask\n",
    "        #     Order of recommended items shouldn't affect their score, thus every item \n",
    "        # should have full attention over the entire sequence: they should know each other and the entire\n",
    "        # conversation history\n",
    "        inductive_attention_mask = self.compute_inductive_attention_mask(\n",
    "            lm_wte_inputs_trimmed.shape[1]-total_wtes.shape[0], \n",
    "            total_wtes.shape[0]\n",
    "        )\n",
    "        rerank_lm_outputs = self.language_model(inputs_embeds=lm_wte_inputs_trimmed,\n",
    "                  inductive_attention_mask=inductive_attention_mask,\n",
    "                  position_ids=combined_position_ids_trimmed,\n",
    "                  output_hidden_states=True)\n",
    "        \n",
    "        rerank_lm_hidden = rerank_lm_outputs.hidden_states[-1][:, -total_wtes.shape[0]:, :]\n",
    "        rerank_logits = self.rerank_logits_mapper(rerank_lm_hidden).squeeze() # torch.Size([num_samples])\n",
    "        \n",
    "        return rerank_logits, gt_item_id_index\n",
    "    \n",
    "    def validation_perform_recall(self, past_wtes, topk):\n",
    "        REC_wtes = self.get_rec_token_wtes()\n",
    "        lm_wte_inputs = torch.cat(\n",
    "            (past_wtes, # [batch (1), len, self.language_model.config.n_embd]\n",
    "             REC_wtes # [1, 1, self.language_model.config.n_embd]\n",
    "            ),\n",
    "            dim=1\n",
    "        )\n",
    "        lm_wte_inputs = self.trim_lm_wtes(lm_wte_inputs) # trim for len > self.language_model.config.n_positions\n",
    "        lm_outputs = self.language_model(inputs_embeds=lm_wte_inputs, output_hidden_states=True)\n",
    "        \n",
    "        rec_token_hidden = lm_outputs.hidden_states[-1][:, -1, :]\n",
    "        # [batch (1), self.recall_lm_query_mapper.out_features]\n",
    "        rec_query_vector = self.recall_lm_query_mapper(rec_token_hidden).squeeze(0) # [768]\n",
    "        rec_query_vector = rec_query_vector.cpu().detach().numpy()\n",
    "        recall_results = self.annoy_base_recall.get_nns_by_vector(rec_query_vector, topk)\n",
    "        return recall_results\n",
    "    \n",
    "    def validation_perform_rerank(self, past_wtes, recalled_ids):\n",
    "        REC_wtes = self.get_rec_token_wtes()\n",
    "        \n",
    "        total_wtes = [ self.annoy_base_rerank.get_item_vector(r_id) for r_id in recalled_ids]\n",
    "        total_wtes = [ torch.tensor(wte).reshape(-1, self.language_model.config.n_embd).to(self.device) for wte in total_wtes]\n",
    "        total_wtes = torch.cat(total_wtes, dim=0) # [len(recalled_ids), 768]\n",
    "        \n",
    "        past_wtes_len = past_wtes.shape[1]\n",
    "        REC_wtes_len = REC_wtes.shape[1] # 1 by default\n",
    "        total_wtes_len = total_wtes.shape[0]\n",
    "        \n",
    "        # compute positional ids, all rerank item wte should have the same positional encoding id 0\n",
    "        position_ids = torch.arange(0, past_wtes_len + REC_wtes_len, dtype=torch.long, device=self.device)\n",
    "        items_position_ids = torch.zeros(total_wtes.shape[0], dtype=torch.long, device=device)\n",
    "        combined_position_ids = torch.cat((position_ids, items_position_ids), dim=0)\n",
    "        combined_position_ids = combined_position_ids.unsqueeze(0) # [1, past_wtes_len+REC_wtes_len+total_wtes_len]\n",
    "        \n",
    "        # compute concatenated lm wtes\n",
    "        lm_wte_inputs = torch.cat(\n",
    "            (past_wtes, # [batch (1), len, self.language_model.config.n_embd]\n",
    "             REC_wtes, # [batch (1), 1, self.language_model.config.n_embd]\n",
    "             total_wtes.unsqueeze(0), # [1, num_samples, self.language_model.config.n_embd]\n",
    "            ),\n",
    "            dim=1\n",
    "        ) # [1, past_len + REC_wtes_len + num_samples, self.language_model.config.n_embd]\n",
    "\n",
    "        # trim sequence to smaller length (len < self.language_model.config.n_positions-self.lm_trim_offset)\n",
    "        combined_position_ids_trimmed = self.trim_positional_ids(combined_position_ids, total_wtes_len) # [1, len]\n",
    "        lm_wte_inputs_trimmed = self.trim_lm_wtes(lm_wte_inputs) # [1, len, self.language_model.config.n_embd]\n",
    "        assert(combined_position_ids.shape[1] == lm_wte_inputs.shape[1])\n",
    "        \n",
    "        inductive_attention_mask = self.compute_inductive_attention_mask(\n",
    "            lm_wte_inputs_trimmed.shape[1]-total_wtes.shape[0], \n",
    "            total_wtes.shape[0]\n",
    "        )\n",
    "        rerank_lm_outputs = self.language_model(inputs_embeds=lm_wte_inputs_trimmed,\n",
    "                  inductive_attention_mask=inductive_attention_mask,\n",
    "                  position_ids=combined_position_ids_trimmed,\n",
    "                  output_hidden_states=True)\n",
    "        \n",
    "        rerank_lm_hidden = rerank_lm_outputs.hidden_states[-1][:, -total_wtes.shape[0]:, :]\n",
    "        rerank_logits = self.rerank_logits_mapper(rerank_lm_hidden).squeeze()\n",
    "        \n",
    "        return rerank_logits\n",
    "    \n",
    "    # TODO: Modify\n",
    "    def validation_generate_sentence(self, past_wtes, recommended_id):\n",
    "        if recommended_id == None: #  pure language\n",
    "            lm_wte_inputs = self.trim_lm_wtes(past_wtes)\n",
    "            lm_outputs = self.language_model(inputs_embeds=lm_wte_inputs, output_hidden_states=True)\n",
    "            generated = model.language_model.generate(\n",
    "                past_key_values=lm_outputs.past_key_values, \n",
    "                max_length=50, \n",
    "                num_return_sequences=1, \n",
    "                do_sample=True, \n",
    "                eos_token_id=198,\n",
    "                pad_token_id=198\n",
    "            )\n",
    "            generated_sen = gpt_tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "            return generated_sen, None\n",
    "        else:\n",
    "            REC_wtes = self.get_rec_token_wtes() # [1, 1, self.language_model.config.n_embd]\n",
    "            gt_item_wte, _ = self.compute_encoded_embeddings_for_items([recommended_id], self.items_db)\n",
    "            gt_item_wte = self.rerank_item_wte_mapper(gt_item_wte) # [1, self.rerank_item_wte_mapper.out_features]\n",
    "            REC_END_wtes = self.get_rec_end_token_wtes() # [1, 1, self.language_model.config.n_embd]\n",
    "\n",
    "            REC_wtes_len = REC_wtes.shape[1] # 1 by default\n",
    "            gt_item_wte_len = gt_item_wte.shape[0] # 1 by default\n",
    "            REC_END_wtes_len = REC_END_wtes.shape[1] # 1 by default\n",
    "\n",
    "            lm_wte_inputs = torch.cat(\n",
    "                (past_wtes, # [batch (1), len, self.language_model.config.n_embd]\n",
    "                 REC_wtes,\n",
    "                 gt_item_wte.unsqueeze(0), # reshape to [1,1,self.rerank_item_wte_mapper.out_features]\n",
    "                 REC_END_wtes\n",
    "                ),\n",
    "                dim=1\n",
    "            )\n",
    "            lm_wte_inputs = self.trim_lm_wtes(lm_wte_inputs) # trim for len > self.language_model.config.n_positions\n",
    "            lm_outputs = self.language_model(inputs_embeds=lm_wte_inputs, output_hidden_states=True)\n",
    "            generated = model.language_model.generate(\n",
    "                past_key_values=lm_outputs.past_key_values, \n",
    "                max_length=50, \n",
    "                num_return_sequences=1, \n",
    "                do_sample=True, \n",
    "                eos_token_id=198,\n",
    "                pad_token_id=198\n",
    "            )\n",
    "            generated_sen = gpt_tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "            return generated_sen, self.get_movie_title(recommended_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef49bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(3)\n",
    "model = UniversalCRSModel(\n",
    "    gpt2_model, \n",
    "    bert_model_recall, \n",
    "    bert_model_rerank, \n",
    "    gpt_tokenizer, \n",
    "    bert_tokenizer, \n",
    "    device, \n",
    "    items_db, \n",
    "    rec_token_str=REC_TOKEN, \n",
    "    rec_end_token_str=REC_END_TOKEN\n",
    ")\n",
    "# model.load_state_dict(torch.load())\n",
    "\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b522611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.420650720596313\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.annoy_base_constructor()\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "# model.annoy_base_recall.save('/local-scratch1/data/by2299/INITIAL_RECALL_ANNOY_BASE_REDIAL_TRAIN_BERT_DISTIL_MULTIPLE.ann')\n",
    "# model.annoy_base_rerank.save('/local-scratch1/data/by2299/INITIAL_RERANK_ANNOY_BASE_REDIAL_TRAIN_BERT_DISTIL_MULTIPLE.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d3f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "num_gradients_accumulation = 1\n",
    "num_train_optimization_steps = len(train_dataset) * num_epochs // batch_size // num_gradients_accumulation\n",
    "\n",
    "num_samples_recall_train = 100\n",
    "num_samples_rerank_train = 150\n",
    "rerank_encoder_chunk_size = int(num_samples_rerank_train / 15)\n",
    "validation_recall_size = 500\n",
    "\n",
    "temperature = 1.2\n",
    "\n",
    "language_loss_train_coeff = 0.15\n",
    "language_loss_train_coeff_beginnging_turn = 1.0\n",
    "recall_loss_train_coeff = 0.8\n",
    "rerank_loss_train_coeff = 1.0\n",
    "\n",
    "# loss\n",
    "criterion_language = SequenceCrossEntropyLoss()\n",
    "criterion_recall = torch.nn.CrossEntropyLoss()\n",
    "rerank_class_weights = torch.FloatTensor([1] * (num_samples_rerank_train-1) + [30]).to(model.device)\n",
    "criterion_rerank_train = torch.nn.CrossEntropyLoss(weight=rerank_class_weights)\n",
    "\n",
    "# optimizer and scheduler\n",
    "param_optimizer = list(model.language_model.named_parameters()) + \\\n",
    "    list(model.recall_encoder.named_parameters()) + \\\n",
    "    list(model.item_encoder.named_parameters()) + \\\n",
    "    list(model.recall_lm_query_mapper.named_parameters()) + \\\n",
    "    list(model.recall_item_wte_mapper.named_parameters()) + \\\n",
    "    list(model.rerank_item_wte_mapper.named_parameters()) + \\\n",
    "    list(model.rerank_logits_mapper.named_parameters())\n",
    "\n",
    "no_decay = ['bias', 'ln', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, \n",
    "                  lr=3e-5,\n",
    "                  eps=1e-06)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(train_dataset) // num_gradients_accumulation , num_training_steps = num_train_optimization_steps)\n",
    "\n",
    "update_count = 0\n",
    "progress_bar = tqdm.notebook.tqdm\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4007e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def past_wtes_constructor(past_list, model):\n",
    "    past_wtes = []\n",
    "    for language_tokens, recommended_ids in past_list:\n",
    "        if language_tokens == None and recommended_ids != None: # rec turn\n",
    "            # append REC, gt_item_wte, REC_END\n",
    "            REC_wte = model.get_rec_token_wtes() # [1, 1, 768]\n",
    "            gt_item_wte, _ = model.compute_encoded_embeddings_for_items(\n",
    "                model.item_encoder,\n",
    "                recommended_ids, \n",
    "                model.items_db\n",
    "            ) # [1, 768]\n",
    "            gt_item_wte = model.rerank_item_wte_mapper(gt_item_wte)\n",
    "            \n",
    "            REC_END_wte = model.get_rec_end_token_wtes() # [1, 1, 768]\n",
    "            combined_wtes = torch.cat(\n",
    "                (REC_wte,\n",
    "                 gt_item_wte.unsqueeze(0), # [1, 1, 768]\n",
    "                 REC_END_wte\n",
    "                ), \n",
    "                dim=1\n",
    "            ) # [1, 3, 768]\n",
    "            past_wtes.append(combined_wtes)\n",
    "        elif recommended_ids == None and language_tokens != None: # language turn simply append wtes\n",
    "            wtes = model.language_model.transformer.wte(language_tokens) # [1, len, 768]\n",
    "            past_wtes.append(wtes)\n",
    "        elif recommended_ids != None and language_tokens != None: # user mentioned turn\n",
    "            l_wtes = model.language_model.transformer.wte(language_tokens)\n",
    "            \n",
    "            SEP_wte = model.get_sep_token_wtes()\n",
    "            gt_item_wte, _ = model.compute_encoded_embeddings_for_items(\n",
    "                model.item_encoder,\n",
    "                recommended_ids, \n",
    "                model.items_db\n",
    "            ) # [1, 768]\n",
    "            gt_item_wte = model.rerank_item_wte_mapper(gt_item_wte)\n",
    "            SEP_wte = model.get_sep_token_wtes()\n",
    "            combined_wtes = torch.cat(\n",
    "                (l_wtes,\n",
    "                 SEP_wte,\n",
    "                 gt_item_wte.unsqueeze(0), # [1, 1, 768]\n",
    "                 SEP_wte\n",
    "                ), \n",
    "                dim=1\n",
    "            )\n",
    "            past_wtes.append(combined_wtes)\n",
    "            \n",
    "    \n",
    "    past_wtes = torch.cat(past_wtes, dim=1)\n",
    "    # don't trim since we already dealt with length in model functions\n",
    "    return past_wtes\n",
    "\n",
    "def train_one_iteration(batch, model):\n",
    "    role_ids, dialogues = batch\n",
    "    dialog_tensors = [torch.LongTensor(utterance).to(model.device) for utterance, _ in dialogues]\n",
    "    \n",
    "    past_list = []\n",
    "    ppl_history = []\n",
    "#     language_logits, language_targets = [], []\n",
    "    for turn_num in range(len(role_ids)):\n",
    "        current_tokens = dialog_tensors[turn_num]\n",
    "        _, recommended_ids = dialogues[turn_num]\n",
    "        \n",
    "        if past_list == []:\n",
    "            past_list.append((current_tokens, recommended_ids))\n",
    "            continue\n",
    "        \n",
    "        if recommended_ids == None: # no rec\n",
    "            if role_ids[turn_num] == 0: # user\n",
    "                past_list.append((current_tokens, None))\n",
    "            else: #system\n",
    "                past_wtes = past_wtes_constructor(past_list, model)\n",
    "                language_logits, language_targets = model.forward_pure_language_turn(past_wtes, current_tokens)\n",
    "                \n",
    "                # loss backward\n",
    "                language_targets_mask = torch.ones_like(language_targets).float()\n",
    "                loss_ppl = criterion_language(language_logits, language_targets, language_targets_mask, label_smoothing=0.02, reduce=\"batch\")\n",
    "                loss_ppl = language_loss_train_coeff * loss_ppl\n",
    "                loss_ppl.backward()\n",
    "                perplexity = np.exp(loss_ppl.item())\n",
    "                ppl_history.append(perplexity)\n",
    "                \n",
    "                # append to past list\n",
    "                past_list.append((current_tokens, None))\n",
    "        else: # rec!\n",
    "            \n",
    "            if role_ids[turn_num] == 0: #user mentioned\n",
    "                past_list.append((current_tokens, recommended_ids))\n",
    "                continue\n",
    "            for recommended_id in recommended_ids:\n",
    "                #system recommend turn\n",
    "                past_wtes = past_wtes_constructor(past_list, model)\n",
    "\n",
    "                # recall\n",
    "                recall_logits, recall_true_index, all_wte_logits, all_wte_targets = model.forward_recall(\n",
    "                    past_wtes, \n",
    "                    current_tokens, \n",
    "                    recommended_id, \n",
    "                    num_samples_recall_train\n",
    "                )\n",
    "                \n",
    "                # recall items loss\n",
    "                recall_targets = torch.LongTensor([recall_true_index]).to(model.device)\n",
    "                loss_recall = criterion_recall(recall_logits.unsqueeze(0), recall_targets)\n",
    "\n",
    "                # language loss in recall turn, REC_TOKEN, Language on conditional generation\n",
    "                all_wte_targets_mask = torch.ones_like(all_wte_targets).float()\n",
    "                loss_ppl = criterion_language(all_wte_logits, all_wte_targets, all_wte_targets_mask, label_smoothing=0.02, reduce=\"batch\")\n",
    "                perplexity = np.exp(loss_ppl.item())\n",
    "                ppl_history.append(perplexity)\n",
    "\n",
    "                # combined loss\n",
    "                recall_total_loss = loss_recall * recall_loss_train_coeff + loss_ppl * language_loss_train_coeff\n",
    "                recall_total_loss.backward()\n",
    "\n",
    "                # rerank\n",
    "                past_wtes = past_wtes_constructor(past_list, model)\n",
    "                rerank_logits, rerank_true_index = model.forward_rerank(\n",
    "                    past_wtes, \n",
    "                    recommended_id, \n",
    "                    num_samples_rerank_train, \n",
    "                    rerank_encoder_chunk_size\n",
    "                )\n",
    "                \n",
    "                rerank_logits /= temperature\n",
    "\n",
    "                # rerank loss \n",
    "                rerank_targets = torch.LongTensor([rerank_true_index]).to(model.device)\n",
    "                loss_rerank = criterion_rerank_train(rerank_logits.unsqueeze(0), rerank_targets)\n",
    "                loss_rerank *= rerank_loss_train_coeff\n",
    "                loss_rerank.backward()\n",
    "\n",
    "            past_list.append((None, recommended_ids))\n",
    "            past_list.append((current_tokens, None))\n",
    "    return np.mean(ppl_history)\n",
    "\n",
    "\n",
    "def validate_one_iteration(batch, model):\n",
    "    role_ids, dialogues = batch\n",
    "    dialog_tensors = [torch.LongTensor(utterance).to(model.device) for utterance, _ in dialogues]\n",
    "    \n",
    "    past_list = []\n",
    "    ppl_history = []\n",
    "    recall_loss_history = []\n",
    "    rerank_loss_history = []\n",
    "    total = 0\n",
    "    recall_top100, recall_top300, recall_top500 = 0, 0, 0,\n",
    "    rerank_top1, rerank_top10, rerank_top50 = 0, 0, 0\n",
    "    \n",
    "    for turn_num in range(len(role_ids)):\n",
    "        current_tokens = dialog_tensors[turn_num]\n",
    "        _, recommended_ids = dialogues[turn_num]\n",
    "        \n",
    "        if past_list == []:\n",
    "            past_list.append((current_tokens, None))\n",
    "            continue\n",
    "        \n",
    "        if recommended_ids == None: # no rec\n",
    "            if role_ids[turn_num] == 0: # user\n",
    "                past_list.append((current_tokens, None))\n",
    "            else: #system\n",
    "                past_wtes = past_wtes_constructor(past_list, model)\n",
    "                language_logits, language_targets = model.forward_pure_language_turn(past_wtes, current_tokens)\n",
    "                \n",
    "                # loss backward\n",
    "                language_targets_mask = torch.ones_like(language_targets).float()\n",
    "                loss_ppl = criterion_language(language_logits, language_targets, language_targets_mask, label_smoothing=-1, reduce=\"sentence\")\n",
    "                perplexity = np.exp(loss_ppl.item())\n",
    "                ppl_history.append(perplexity)\n",
    "                del loss_ppl\n",
    "                \n",
    "                # append to past list\n",
    "                past_list.append((current_tokens, None))\n",
    "        else: # rec!\n",
    "            \n",
    "            if role_ids[turn_num] == 0: #user mentioned\n",
    "                past_list.append((current_tokens, recommended_ids))\n",
    "                continue\n",
    "            for recommended_id in recommended_ids:\n",
    "                past_wtes = past_wtes_constructor(past_list, model)\n",
    "\n",
    "                total += 1\n",
    "\n",
    "                # recall\n",
    "                recall_logits, recall_true_index, all_wte_logits, all_wte_targets = model.forward_recall(\n",
    "                    past_wtes, \n",
    "                    current_tokens, \n",
    "                    recommended_id, \n",
    "                    num_samples_recall_train\n",
    "                )\n",
    "\n",
    "                # recall items loss\n",
    "                recall_targets = torch.LongTensor([recall_true_index]).to(model.device)\n",
    "                loss_recall = criterion_recall(recall_logits.unsqueeze(0), recall_targets)\n",
    "                recall_loss_history.append(loss_recall.item())\n",
    "                del loss_recall; del recall_logits; del recall_targets\n",
    "\n",
    "                # language loss in recall turn, REC_TOKEN, Language on conditional generation\n",
    "                all_wte_targets_mask = torch.ones_like(all_wte_targets).float()\n",
    "                loss_ppl = criterion_language(all_wte_logits, all_wte_targets, all_wte_targets_mask, label_smoothing=-1, reduce=\"sentence\")\n",
    "                perplexity = np.exp(loss_ppl.item())\n",
    "                ppl_history.append(perplexity)\n",
    "                del loss_ppl; del all_wte_logits; del all_wte_targets\n",
    "\n",
    "                recalled_ids = model.validation_perform_recall(past_wtes, validation_recall_size)\n",
    "\n",
    "                if recommended_id in recalled_ids[:500]:\n",
    "                    recall_top500 += 1\n",
    "                if recommended_id in recalled_ids[:400]:\n",
    "                    recall_top300 += 1\n",
    "                if recommended_id in recalled_ids[:300]:\n",
    "                    recall_top100 += 1\n",
    "\n",
    "                if recommended_id not in recalled_ids:\n",
    "                    continue # no need to compute rerank since recall is unsuccessful\n",
    "\n",
    "                # rerank\n",
    "                past_wtes = past_wtes_constructor(past_list, model)\n",
    "                rerank_true_index = recalled_ids.index(recommended_id)\n",
    "                rerank_logits = model.validation_perform_rerank(past_wtes, recalled_ids)\n",
    "    #             print(rerank_logits)\n",
    "                reranks = np.argsort(rerank_logits.cpu().detach().numpy())\n",
    "                if rerank_true_index in reranks[-50:]:\n",
    "                    rerank_top50 += 1\n",
    "                if rerank_true_index in reranks[-10:]:\n",
    "                    rerank_top10 += 1\n",
    "                if rerank_true_index in reranks[-1:]:\n",
    "                    rerank_top1 += 1\n",
    "                    \n",
    "    #             print(recalled_ids[reranks[-1]],recalled_ids[reranks[-2]],recalled_ids[reranks[-3]],\n",
    "    #                  recalled_ids[reranks[-4]],recalled_ids[reranks[-5]],recalled_ids[reranks[-6]],\n",
    "    #                  recalled_ids[reranks[-7]],recalled_ids[reranks[-8]],recalled_ids[reranks[-9]],)\n",
    "    #             print(recommended_id)\n",
    "                rerank_targets = torch.LongTensor([rerank_true_index]).to(model.device)\n",
    "    #             loss_rerank = criterion_rerank(rerank_logits.unsqueeze(0), rerank_targets)\n",
    "                rerank_loss_val = torch.nn.CrossEntropyLoss()\n",
    "                loss_rerank = rerank_loss_val(rerank_logits.unsqueeze(0), rerank_targets)\n",
    "                rerank_loss_history.append(loss_rerank.item())\n",
    "                del loss_rerank; del rerank_logits; del rerank_targets\n",
    "            \n",
    "            past_list.append((None, recommended_ids))\n",
    "            past_list.append((current_tokens, None))\n",
    "    return ppl_history, recall_loss_history, rerank_loss_history, \\\n",
    "            total, recall_top100, recall_top300, recall_top500, \\\n",
    "            rerank_top1, rerank_top10, rerank_top50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27a22444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_metrics(outs):\n",
    "    # outputs is a list which contains several sentences, each sentence contains several words\n",
    "    unigram_count = 0\n",
    "    bigram_count = 0\n",
    "    trigram_count=0\n",
    "    quagram_count=0\n",
    "    unigram_set = set()\n",
    "    bigram_set = set()\n",
    "    trigram_set=set()\n",
    "    quagram_set=set()\n",
    "    for sen in outs:\n",
    "        for word in sen:\n",
    "            unigram_count += 1\n",
    "            unigram_set.add(word)\n",
    "        for start in range(len(sen) - 1):\n",
    "            bg = str(sen[start]) + ' ' + str(sen[start + 1])\n",
    "            bigram_count += 1\n",
    "            bigram_set.add(bg)\n",
    "        for start in range(len(sen)-2):\n",
    "            trg=str(sen[start]) + ' ' + str(sen[start + 1]) + ' ' + str(sen[start + 2])\n",
    "            trigram_count+=1\n",
    "            trigram_set.add(trg)\n",
    "        for start in range(len(sen)-3):\n",
    "            quag=str(sen[start]) + ' ' + str(sen[start + 1]) + ' ' + str(sen[start + 2]) + ' ' + str(sen[start + 3])\n",
    "            quagram_count+=1\n",
    "            quagram_set.add(quag)\n",
    "    dis1 = len(unigram_set) / len(outs)#unigram_count\n",
    "    dis2 = len(bigram_set) / len(outs)#bigram_count\n",
    "    dis3 = len(trigram_set)/len(outs)#trigram_count\n",
    "    dis4 = len(quagram_set)/len(outs)#quagram_count\n",
    "    return dis1, dis2, dis3, dis4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dcb51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_language_metrics_batch(batch, model, item_id_2_lm_token_id):\n",
    "    role_ids, dialogues = batch\n",
    "    dialog_tensors = [torch.LongTensor(utterance).to(model.device) for utterance, _ in dialogues]\n",
    "    \n",
    "#     past_list = []\n",
    "    past_tokens = None\n",
    "    tokenized_sentences = []\n",
    "    integration_total, integration_cnt = 0, 0\n",
    "    \n",
    "    for turn_num in range(len(role_ids)):\n",
    "        dial_turn_inputs = dialog_tensors[turn_num]\n",
    "        _, recommended_ids = dialogues[turn_num]\n",
    "        \n",
    "        item_ids = []; \n",
    "        if recommended_ids != None:\n",
    "            for r_id in recommended_ids:\n",
    "                item_ids.append(item_id_2_lm_token_id[r_id])\n",
    "            item_ids = torch.tensor([item_ids]).to(device)\n",
    "        \n",
    "        if turn_num == 0:\n",
    "            past_tokens = dial_turn_inputs\n",
    "        if role_ids[turn_num] == 0:\n",
    "            if turn_num != 0:\n",
    "                past_tokens = torch.cat((past_tokens, dial_turn_inputs), dim=1)\n",
    "        else:\n",
    "            if turn_num != 0:\n",
    "                if item_ids != []:\n",
    "                    rec_start_token = model.lm_tokenizer(model.REC_TOKEN_STR, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "                    rec_end_token = model.lm_tokenizer(model.REC_END_TOKEN_STR, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "                    past_tokens = torch.cat((past_tokens, rec_start_token, item_ids, rec_end_token), dim=1)\n",
    "                else:\n",
    "                    past_tokens = past_tokens\n",
    "                \n",
    "            total_len = past_tokens.shape[1]\n",
    "            if total_len >= 1024: break\n",
    "#                 print(\"Original Rec: \" + gpt_tokenizer.decode(dial_turn_inputs[0], skip_special_tokens=True))\n",
    "            generated = model.language_model.generate(\n",
    "                input_ids= torch.cat((past_tokens, torch.tensor([[32, 25]]).to(device)), dim=1),\n",
    "                max_length=1024,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                num_beams=2,\n",
    "                top_k=50,\n",
    "                temperature=1.05,\n",
    "                eos_token_id=628,\n",
    "                pad_token_id=628,\n",
    "#                 no_repeat_ngram_size=3,\n",
    "#                         length_penalty=3.0\n",
    "\n",
    "            )\n",
    "            generated_sen =  gpt_tokenizer.decode(generated[0][past_tokens.shape[1]:], skip_special_tokens=True)\n",
    "#                 print(\"Generated Rec: \" + generated_sen)\n",
    "            tokenized_sen = generated_sen.strip().split(' ')\n",
    "            tokenized_sentences.append(tokenized_sen)\n",
    "            if recommended_ids != None:\n",
    "                integration_total += 1                        \n",
    "                if \"[MOVIE_ID]\" in generated_sen:\n",
    "                    integration_cnt += 1\n",
    "            \n",
    "            if turn_num != 0:\n",
    "                past_tokens = torch.cat((past_tokens, dial_turn_inputs), dim=1)\n",
    "            \n",
    "    return tokenized_sentences, integration_cnt, integration_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "979f0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"Outputs/CRS_Train.txt\"\n",
    "model_saved_path = \"/local-scratch1/data/by2299/CRS_Redial_Train_Same_BERT_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ddde737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a636e7c1e2b44de9d93f978919f8adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2142549/160314524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mppl_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrerank_loss_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_top100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_top300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_top500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrerank_top1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrerank_top10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrerank_top50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_one_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mppls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mppl_history\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mrecall_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrecall_loss_history\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mrerank_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrerank_loss_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtotal_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2142549/4029912764.py\u001b[0m in \u001b[0;36mvalidate_one_iteration\u001b[0;34m(batch, model)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mloss_ppl\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mall_wte_logits\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mall_wte_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mrecalled_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_perform_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_wtes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_recall_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrecommended_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecalled_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2142549/3797519886.py\u001b[0m in \u001b[0;36mvalidation_perform_recall\u001b[0;34m(self, past_wtes, topk)\u001b[0m\n\u001b[1;32m    375\u001b[0m         )\n\u001b[1;32m    376\u001b[0m         \u001b[0mlm_wte_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrim_lm_wtes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_wte_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# trim for len > self.language_model.config.n_positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mlm_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_wte_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mrec_token_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/please/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GoodConvRecIntegration/utils/InductiveAttentionModels.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, inductive_attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/please/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GoodConvRecIntegration/utils/InductiveAttentionModels.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, inductive_attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    393\u001b[0m                 )\n\u001b[1;32m    394\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    396\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/please/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GoodConvRecIntegration/utils/InductiveAttentionModels.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, inductive_attention_mask)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0minductive_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     ):\n\u001b[0;32m---> 68\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/please/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GoodConvRecIntegration/utils/InductiveAttentionModels.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, inductive_attention_mask)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/please/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/please/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1768\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "\n",
    "    #\"Training\"\n",
    "    pbar = progress_bar(train_dataloader)\n",
    "    model.train()\n",
    "    for batch in pbar:\n",
    "        # batch size of train_dataloader is 1\n",
    "        avg_ppl = train_one_iteration(batch[0], model)\n",
    "        update_count +=1\n",
    "        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:\n",
    "            \n",
    "            # update for gradient accumulation\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # speed measure\n",
    "            end = time.time()\n",
    "            speed = batch_size * num_gradients_accumulation / (end - start)\n",
    "            start = end\n",
    "            \n",
    "            # show progress\n",
    "            pbar.set_postfix(ppl=avg_ppl, speed=speed)\n",
    "            \n",
    "    model.eval()\n",
    "    model.annoy_base_constructor()\n",
    "    \n",
    "    pbar = progress_bar(test_dataloader)\n",
    "    ppls, recall_losses, rerank_losses = [],[],[]\n",
    "    total_val, recall_top100_val, recall_top300_val, recall_top500_val, \\\n",
    "        rerank_top1_val, rerank_top10_val, rerank_top50_val = 0,0,0,0,0,0,0\n",
    "    for batch in pbar:\n",
    "        ppl_history, recall_loss_history, rerank_loss_history, \\\n",
    "        total, recall_top100, recall_top300, recall_top500, \\\n",
    "        rerank_top1, rerank_top10, rerank_top50 = validate_one_iteration(batch[0], model)\n",
    "        ppls += ppl_history; recall_losses += recall_loss_history; rerank_losses += rerank_loss_history\n",
    "        total_val += total; \n",
    "        recall_top100_val += recall_top100; recall_top300_val += recall_top300; recall_top500_val += recall_top500\n",
    "        rerank_top1_val += rerank_top1; rerank_top10_val += rerank_top10; rerank_top50_val += rerank_top50\n",
    "    \n",
    "    item_id_2_lm_token_id = model.lm_expand_wtes_with_items_annoy_base()\n",
    "    pbar = progress_bar(test_dataloader)\n",
    "    total_sentences = []\n",
    "    integration_cnt, total_int_cnt = 0, 0\n",
    "    for batch in pbar:\n",
    "        sentences, ic, tc = validate_language_metrics_batch(batch[0], model, item_id_2_lm_token_id)\n",
    "        for s in sentences:\n",
    "            total_sentences.append(s)\n",
    "\n",
    "        integration_cnt += ic; total_int_cnt += tc\n",
    "    integration_ratio = integration_cnt / total_int_cnt\n",
    "    dist1, dist2, dist3, dist4 = distinct_metrics(total_sentences)\n",
    "    model.lm_restore_wtes(original_token_emb_size)\n",
    "    \n",
    "    output_file = open(output_file_path, 'a')\n",
    "    output_file.writelines([f\"Epcoh {ep} ppl: {np.mean(ppls)}, recall_loss: {np.mean(recall_losses)}, rerank_loss: {np.mean(rerank_losses)}\"])\n",
    "    output_file.write('\\n')\n",
    "    output_file.writelines([f\"recall top100: {recall_top100_val/total_val}, top300: {recall_top300_val/total_val}, top500: {recall_top500_val/total_val}\"])\n",
    "    output_file.write('\\n')\n",
    "    output_file.writelines([f\"rerank top1: {rerank_top1_val/total_val}, top10: {rerank_top10_val/total_val}, top50: {rerank_top50_val/total_val}\"])\n",
    "    output_file.write('\\n')\n",
    "    output_file.writelines([f\"Integration Ratio: {integration_ratio}\"])\n",
    "    output_file.write('\\n')\n",
    "    output_file.writelines([f\"Dist1: {dist1}, Dist2: {dist2}, Dist3: {dist3}, Dist4: {dist4}\"])\n",
    "    output_file.write('\\n\\n')\n",
    "    output_file.close()\n",
    "    \n",
    "    torch.save(model.state_dict(), model_saved_path + str(ep) +\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "905e2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_metrics(outs):\n",
    "    # outputs is a list which contains several sentences, each sentence contains several words\n",
    "    unigram_count = 0\n",
    "    bigram_count = 0\n",
    "    trigram_count=0\n",
    "    quagram_count=0\n",
    "    unigram_set = set()\n",
    "    bigram_set = set()\n",
    "    trigram_set=set()\n",
    "    quagram_set=set()\n",
    "    for sen in outs:\n",
    "        sen = [ i.translate(str.maketrans('', '', string.punctuation)) for i in sen][1:]\n",
    "        for i in range(len(sen)):\n",
    "            sen[i] = sen[i].lower()\n",
    "        if len(sen) >= 100: continue\n",
    "        for word in sen:\n",
    "            unigram_count += 1\n",
    "            unigram_set.add(word)\n",
    "        for start in range(len(sen) - 1):\n",
    "            bg = str(sen[start]) + ' ' + str(sen[start + 1])\n",
    "            bigram_count += 1\n",
    "            bigram_set.add(bg)\n",
    "        for start in range(len(sen)-2):\n",
    "            trg=str(sen[start]) + ' ' + str(sen[start + 1]) + ' ' + str(sen[start + 2])\n",
    "            trigram_count+=1\n",
    "            trigram_set.add(trg)\n",
    "        for start in range(len(sen)-3):\n",
    "            quag=str(sen[start]) + ' ' + str(sen[start + 1]) + ' ' + str(sen[start + 2]) + ' ' + str(sen[start + 3])\n",
    "            quagram_count+=1\n",
    "            quagram_set.add(quag)\n",
    "    dis1 = len(unigram_set) / len(outs)#unigram_count\n",
    "    dis2 = len(bigram_set) / len(outs)#bigram_count\n",
    "    dis3 = len(trigram_set)/len(outs)#trigram_count\n",
    "    dis4 = len(quagram_set)/len(outs)#quagram_count\n",
    "    return dis1, dis2, dis3, dis4\n",
    "\n",
    "def bleu_calc_one(ref, hyp):\n",
    "    for i in range(len(ref)):\n",
    "        ref[i] = ref[i].lower()\n",
    "    for i in range(len(hyp)):\n",
    "        hyp[i] = hyp[i].lower()\n",
    "    bleu1 = sentence_bleu([ref], hyp, weights=(1, 0, 0, 0), smoothing_function=bleu_score.SmoothingFunction(epsilon=1e-12).method7)\n",
    "    bleu2 = sentence_bleu([ref], hyp, weights=(1/2, 1/2, 0, 0), smoothing_function=bleu_score.SmoothingFunction(epsilon=1e-12).method7)\n",
    "    bleu3 = sentence_bleu([ref], hyp, weights=(1/3, 1/3, 1/3, 0), smoothing_function=bleu_score.SmoothingFunction(epsilon=1e-12).method7)\n",
    "    bleu4 = sentence_bleu([ref], hyp, weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=bleu_score.SmoothingFunction(epsilon=1e-12).method7)\n",
    "    return bleu1, bleu2, bleu3, bleu4\n",
    "\n",
    "def bleu_calc_all(originals, generated):\n",
    "    bleu1_total, bleu2_total, bleu3_total, bleu4_total = 0, 0, 0, 0\n",
    "    total = 0\n",
    "    for o, g in zip(originals, generated):\n",
    "        r = [ i.translate(str.maketrans('', '', string.punctuation)) for i in o][1:]\n",
    "        h = [ i.translate(str.maketrans('', '', string.punctuation)) for i in g][1:]\n",
    "        if '[MOVIE_ID]' in r: continue\n",
    "#         if len(g) >= 500: continue\n",
    "        if len(g) >= 100: continue\n",
    "        bleu1, bleu2, bleu3, bleu4 = bleu_calc_one(r, h)\n",
    "        bleu1_total += bleu1; bleu2_total += bleu2; bleu3_total += bleu3; bleu4_total += bleu4;\n",
    "        total += 1\n",
    "    return bleu1_total / total, bleu2_total / total, bleu3_total / total, bleu4_total / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb0f4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_placeholder(sentence, movie_titles):\n",
    "    sen = sentence\n",
    "    for title in movie_titles:\n",
    "        sen = sen.replace(\"[MOVIE_ID]\", title, 1)\n",
    "    return sen\n",
    "        \n",
    "\n",
    "def validate_language_metrics_batch(batch, model, item_id_2_lm_token_id):\n",
    "    role_ids, dialogues = batch\n",
    "    dialog_tensors = [torch.LongTensor(utterance).to(model.device) for utterance, _ in dialogues]\n",
    "    \n",
    "#     past_list = []\n",
    "    past_tokens = None\n",
    "    original_sentences = []\n",
    "    tokenized_sentences = []\n",
    "    integration_total, integration_cnt = 0, 0\n",
    "    valid_gen_selected_cnt = 0; total_gen_cnt = 0; response_with_items = 0; original_response_with_items = 0\n",
    "    \n",
    "    for turn_num in range(len(role_ids)):\n",
    "        dial_turn_inputs = dialog_tensors[turn_num]\n",
    "        _, recommended_ids = dialogues[turn_num]\n",
    "        \n",
    "        item_ids = []; item_titles = []\n",
    "        if recommended_ids != None:\n",
    "            for r_id in recommended_ids:\n",
    "                item_ids.append(item_id_2_lm_token_id[r_id])\n",
    "                title = model.items_db[r_id]\n",
    "                title = title.split('[SEP]')[0].strip()\n",
    "                item_titles.append(title)\n",
    "            item_ids = torch.tensor([item_ids]).to(device)\n",
    "        \n",
    "#         if turn_num == 0:\n",
    "#             past_tokens = dial_turn_inputs\n",
    "        if role_ids[turn_num] == 0:\n",
    "            if turn_num == 0:\n",
    "                past_tokens = dial_turn_inputs\n",
    "            elif turn_num != 0:\n",
    "                past_tokens = torch.cat((past_tokens, dial_turn_inputs), dim=1)\n",
    "        else:\n",
    "            if turn_num != 0:\n",
    "                if item_ids != []:\n",
    "                    rec_start_token = model.lm_tokenizer(model.REC_TOKEN_STR, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "                    rec_end_token = model.lm_tokenizer(model.REC_END_TOKEN_STR, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "                    past_tokens = torch.cat((past_tokens, rec_start_token, item_ids, rec_end_token), dim=1)\n",
    "                else:\n",
    "                    past_tokens = past_tokens\n",
    "                \n",
    "                total_len = past_tokens.shape[1]\n",
    "                if total_len >= 1024: break\n",
    "\n",
    "                original_sen = gpt_tokenizer.decode(dial_turn_inputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "                generated = model.language_model.generate(\n",
    "                    input_ids= torch.cat((past_tokens, torch.tensor([[32, 25]]).to(device)), dim=1),\n",
    "                    max_length=1024,\n",
    "                    num_return_sequences=5,\n",
    "                    do_sample=True,\n",
    "                    num_beams=5,\n",
    "                    top_k=50,\n",
    "                    temperature=1.25,\n",
    "                    eos_token_id=628,\n",
    "                    pad_token_id=628,\n",
    "    #                 no_repeat_ngram_size=3,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True\n",
    "                )\n",
    "                # check valid generations, equal num [MOVIE_ID] placeholders\n",
    "                total_gen_cnt += 1\n",
    "                valid_gens = []; valid_gens_scores = []\n",
    "                final_gen = None\n",
    "                if len(item_ids) == 0: # no rec items\n",
    "                    for i in range(len(generated.sequences)):\n",
    "                        gen_sen = gpt_tokenizer.decode(generated.sequences[i][past_tokens.shape[1]:], skip_special_tokens=True)\n",
    "                        if gen_sen.count(\"[MOVIE_ID]\") == 0:\n",
    "                            valid_gens.append(gen_sen); valid_gens_scores.append(generated.sequences_scores[i].item())\n",
    "                    if valid_gens == [] and valid_gens_scores == []: # no valid, pick with highest score\n",
    "                        i = torch.argmax(generated.sequences_scores).item()\n",
    "                        final_gen = gpt_tokenizer.decode(generated.sequences[i][past_tokens.shape[1]:], skip_special_tokens=True)\n",
    "                    else: # yes valid\n",
    "                        i = np.argmax(valid_gens_scores)\n",
    "                        final_gen = valid_gens[i]\n",
    "                        valid_gen_selected_cnt += 1\n",
    "                else:\n",
    "                    original_response_with_items += 1\n",
    "                    for i in range(len(generated.sequences)):\n",
    "                        gen_sen = gpt_tokenizer.decode(generated.sequences[i][past_tokens.shape[1]:], skip_special_tokens=True)\n",
    "                        if gen_sen.count(\"[MOVIE_ID]\") == original_sen.count(\"[MOVIE_ID]\"):\n",
    "                            valid_gens.append(gen_sen); valid_gens_scores.append(generated.sequences_scores[i].item())\n",
    "                    if valid_gens == [] and valid_gens_scores == []: # no valid, pick with highest score\n",
    "                        i = torch.argmax(generated.sequences_scores).item()\n",
    "                        final_gen = gpt_tokenizer.decode(generated.sequences[i][past_tokens.shape[1]:], skip_special_tokens=True)\n",
    "                        if \"[MOVIE_ID]\" in final_gen:\n",
    "                            response_with_items += 1\n",
    "                        final_gen = replace_placeholder(final_gen, item_titles)\n",
    "                    else:\n",
    "                        i = np.argmax(valid_gens_scores)\n",
    "                        final_gen = valid_gens[i]\n",
    "                        if \"[MOVIE_ID]\" in final_gen:\n",
    "                            response_with_items += 1\n",
    "                        final_gen = replace_placeholder(final_gen, item_titles)\n",
    "                        valid_gen_selected_cnt += 1\n",
    "\n",
    "    #             generated_sen =  gpt_tokenizer.decode(generated[0][past_tokens.shape[1]:], skip_special_tokens=True)\n",
    "    #             print(\"Generated Rec: \" + final_gen)\n",
    "                tokenized_sen = final_gen.strip().split(' ')\n",
    "                tokenized_sentences.append(tokenized_sen)\n",
    "                original_sen = replace_placeholder(original_sen, item_titles).replace(\"\\n\\n\\n\", \"\")\n",
    "    #             print(\"Original Rec: \" + original_sen)\n",
    "                original_sentences.append( original_sen.strip().split(' ') )\n",
    "                if recommended_ids != None:\n",
    "                    integration_total += 1                        \n",
    "                    if \"[MOVIE_ID]\" in final_gen:\n",
    "                        integration_cnt += 1\n",
    "\n",
    "                if turn_num != 0:\n",
    "                    past_tokens = torch.cat((past_tokens, dial_turn_inputs), dim=1)\n",
    "            elif turn_num == 0:\n",
    "                original_sen = gpt_tokenizer.decode(dial_turn_inputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "                generated = model.language_model.generate(\n",
    "                    input_ids= torch.tensor([[32, 25]]).to(device),\n",
    "                    max_length=1024,\n",
    "                    num_return_sequences=5,\n",
    "                    do_sample=True,\n",
    "                    num_beams=5,\n",
    "                    top_k=50,\n",
    "                    temperature=1.25,\n",
    "                    eos_token_id=628,\n",
    "                    pad_token_id=628,\n",
    "    #                 no_repeat_ngram_size=3,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True\n",
    "                )\n",
    "                # check valid generations, equal num [MOVIE_ID] placeholders\n",
    "                total_gen_cnt += 1\n",
    "                valid_gens = []; valid_gens_scores = []\n",
    "                final_gen = None\n",
    "                if len(item_ids) == 0: # no rec items\n",
    "                    for i in range(len(generated.sequences)):\n",
    "                        gen_sen = gpt_tokenizer.decode(generated.sequences[i], skip_special_tokens=True)\n",
    "                        if gen_sen.count(\"[MOVIE_ID]\") == 0:\n",
    "                            valid_gens.append(gen_sen); valid_gens_scores.append(generated.sequences_scores[i].item())\n",
    "                    if valid_gens == [] and valid_gens_scores == []: # no valid, pick with highest score\n",
    "                        i = torch.argmax(generated.sequences_scores).item()\n",
    "                        final_gen = gpt_tokenizer.decode(generated.sequences[i], skip_special_tokens=True)\n",
    "                    else: # yes valid\n",
    "                        i = np.argmax(valid_gens_scores)\n",
    "                        final_gen = valid_gens[i]\n",
    "                        valid_gen_selected_cnt += 1\n",
    "                else:\n",
    "                    original_response_with_items += 1\n",
    "                    for i in range(len(generated.sequences)):\n",
    "                        gen_sen = gpt_tokenizer.decode(generated.sequences[i], skip_special_tokens=True)\n",
    "                        if gen_sen.count(\"[MOVIE_ID]\") == original_sen.count(\"[MOVIE_ID]\"):\n",
    "                            valid_gens.append(gen_sen); valid_gens_scores.append(generated.sequences_scores[i].item())\n",
    "                    if valid_gens == [] and valid_gens_scores == []: # no valid, pick with highest score\n",
    "                        i = torch.argmax(generated.sequences_scores).item()\n",
    "                        final_gen = gpt_tokenizer.decode(generated.sequences[i], skip_special_tokens=True)\n",
    "                        if \"[MOVIE_ID]\" in final_gen:\n",
    "                            response_with_items += 1\n",
    "                        final_gen = replace_placeholder(final_gen, item_titles)\n",
    "                    else:\n",
    "                        i = np.argmax(valid_gens_scores)\n",
    "                        final_gen = valid_gens[i]\n",
    "                        if \"[MOVIE_ID]\" in final_gen:\n",
    "                            response_with_items += 1\n",
    "                        final_gen = replace_placeholder(final_gen, item_titles)\n",
    "                        valid_gen_selected_cnt += 1\n",
    "\n",
    "    #             generated_sen =  gpt_tokenizer.decode(generated[0][past_tokens.shape[1]:], skip_special_tokens=True)\n",
    "    #             print(\"Generated Rec: \" + final_gen)\n",
    "                tokenized_sen = final_gen.strip().split(' ')\n",
    "                tokenized_sentences.append(tokenized_sen)\n",
    "                original_sen = replace_placeholder(original_sen, item_titles).replace(\"\\n\\n\\n\", \"\")\n",
    "    #             print(\"Original Rec: \" + original_sen)\n",
    "                original_sentences.append( original_sen.strip().split(' ') )\n",
    "                if recommended_ids != None:\n",
    "                    integration_total += 1                        \n",
    "                    if \"[MOVIE_ID]\" in final_gen:\n",
    "                        integration_cnt += 1\n",
    "                \n",
    "                if turn_num == 0:\n",
    "                    past_tokens = dial_turn_inputs\n",
    "            \n",
    "    return original_sentences, tokenized_sentences, integration_cnt, integration_total, valid_gen_selected_cnt, total_gen_cnt, response_with_items, original_response_with_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afaf8b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fdd32071cc40aca0b03e919634d4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/by2299/anaconda3/envs/please/lib/python3.9/site-packages/transformers/generation_utils.py:2259: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.annoy_base_constructor()\n",
    "\n",
    "# pbar = progress_bar(test_dataloader)\n",
    "\n",
    "item_id_2_lm_token_id = model.lm_expand_wtes_with_items_annoy_base()\n",
    "pbar = progress_bar(test_dataloader)\n",
    "total_sentences_original = []; total_sentences_generated = []\n",
    "integration_cnt, total_int_cnt = 0, 0\n",
    "valid_cnt, total_gen_cnt, response_with_items = 0, 0, 0\n",
    "for batch in pbar:\n",
    "    original_sens, sentences, ic, tc, vc, tgc, rwi, group = validate_language_metrics_batch(batch[0], model, item_id_2_lm_token_id)\n",
    "#     for s in original_sens:\n",
    "#         total_sentences_original.append(s)\n",
    "#     for s in sentences:\n",
    "#         total_sentences_generated.append(s)\n",
    "    total_sentences_original.append(original_sens)\n",
    "    total_sentences_generated.append(sentences)\n",
    "    \n",
    "    integration_cnt += ic; total_int_cnt += tc\n",
    "    valid_cnt += vc; total_gen_cnt += tgc; response_with_items += rwi\n",
    "integration_ratio = integration_cnt / total_int_cnt\n",
    "valid_gen_ratio = valid_cnt / total_gen_cnt\n",
    "# dist1, dist2, dist3, dist4 = distinct_metrics(total_sentences_generated)\n",
    "# bleu1, bleu2, bleu3, bleu4 = bleu_calc_all(total_sentences_original, total_sentences_generated)\n",
    "model.lm_restore_wtes(original_token_emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83778baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9145149160190524, 0.4413386813737779)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_cnt / total_gen_cnt, response_with_items / total_gen_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58d229c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(total_sentences_generated, '../human_eval/mese2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2fd3e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9173978440711957, 0.4402105790925044)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_cnt / total_gen_cnt, response_with_items / total_gen_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8b6f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2602155928804212 0.7049385810980195 1.0142892955627978 1.156054148909501\n",
      "0.342754064465557 0.2512299152344354 0.1892696463143165 0.14407982061678162\n"
     ]
    }
   ],
   "source": [
    "dist1, dist2, dist3, dist4 = distinct_metrics(total_sentences_generated)\n",
    "bleu1, bleu2, bleu3, bleu4 = bleu_calc_all(total_sentences_original, total_sentences_generated)\n",
    "print(dist1, dist2, dist3, dist4)\n",
    "print(bleu1, bleu2, bleu3, bleu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ec4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "please"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
